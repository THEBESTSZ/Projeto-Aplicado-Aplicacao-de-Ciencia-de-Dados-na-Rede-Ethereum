{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4592f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Caminho da pasta de saída\n",
    "output_folder = \"../Scripts/output\"\n",
    "\n",
    "# Obter lista de arquivos CSV de blocos\n",
    "block_files = glob.glob(os.path.join(output_folder, \"block\", \"*.csv\"))\n",
    "\n",
    "# Obter lista de arquivos CSV de swaps\n",
    "swap_files = glob.glob(os.path.join(output_folder, \"swap\", \"*.csv\"))\n",
    "\n",
    "# Obter lista de arquivos CSV de transações\n",
    "transaction_files = glob.glob(os.path.join(output_folder, \"transaction\", \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb2d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar sessão Spark\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Análise de Transações Ethereum\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2590d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um DataFrame vazio para armazenar os dados dos blocos\n",
    "combined_block_df = None\n",
    "\n",
    "file_path = \"../Scripts/output/block/block.csv\"\n",
    "\n",
    "# Ler o arquivo CSV\n",
    "combined_block_df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51b7a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_block_df = combined_block_df.drop(\"Gas_Limit\")\n",
    "combined_block_df = combined_block_df.drop(\"Gas_Used\")\n",
    "combined_block_df = combined_block_df.drop(\"Timestamp_Block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5ffc4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um DataFrame vazio para armazenar os dados dos blocos\n",
    "combined_swap_df = None\n",
    "\n",
    "file_path = \"../Scripts/output/swap/swap_transactions.csv\"\n",
    "\n",
    "# Ler o arquivo CSV\n",
    "combined_swap_df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "193a83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_swap_df = combined_swap_df.drop(\"Block_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a20c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_transaction_df = None\n",
    "\n",
    "file_path = \"../Scripts/output/transaction/transactions.csv\"\n",
    "\n",
    "# Ler o arquivo CSV\n",
    "combined_transaction_df = spark.read.csv(file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bcc6f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_swap_df_cleaned = combined_swap_df.dropna(subset=['From_Token_Symbol', 'To_Token_Symbol', 'From_Token_Holders_Count', 'To_Token_Holders_Count', 'From_Token_Price', 'To_Token_Price'])\n",
    "\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "combined_swap_df_cleaned = combined_swap_df_cleaned.withColumn(\"From_Token_Price\", when(col(\"From_Token_Price\") == 0, float('nan')).otherwise(col(\"From_Token_Price\")))\n",
    "combined_swap_df_cleaned = combined_swap_df_cleaned.withColumn(\"To_Token_Price\", when(col(\"To_Token_Price\") == 0, float('nan')).otherwise(col(\"To_Token_Price\")))\n",
    "df = combined_block_df.join(combined_transaction_df, \"Block_ID\").join(combined_swap_df_cleaned, \"Hash_Transaction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47915dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\pandas\\conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 3s 8ms/step - loss: 6.4939e-05\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.0483e-05\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 3.1061e-05\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 4.6696e-05\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.8320e-05\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.2481e-05\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 3.4007e-05\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 2.0611e-05\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.8410e-05\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 2.5842e-05\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.9536e-05\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.5851e-05\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.9795e-05\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.7703e-05\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.4592e-05\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.6257e-05\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.5861e-05\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.3753e-05\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 1.4720e-05\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 1.4616e-05\n",
      "1/1 [==============================] - 1s 553ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "Previsões de preço para as próximas 10 horas:\n",
      "[2.7419849e-06 2.7574235e-06 2.7608539e-06 2.7616161e-06 2.7617855e-06\n",
      " 2.7618232e-06 2.7618314e-06 2.7618332e-06 2.7618337e-06 2.7618339e-06]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Função para pré-processar os dados e criar as janelas de entrada\n",
    "def prepare_data(df, window_size):\n",
    "    data = df.values\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Filtrar os dados para a criptomoeda mais negociada (exemplo: Ethereum)\n",
    "most_traded_coin = \"PEPIG\"\n",
    "df_filtered = df.filter(col(\"To_Token_Symbol\") == most_traded_coin)\n",
    "\n",
    "# Converter a coluna de timestamp para formato de data\n",
    "df_filtered = df_filtered.withColumn(\"Date\", to_timestamp(col(\"Timestamp_Transaction\")))\n",
    "\n",
    "# Selecionar apenas as colunas relevantes (data e preço)\n",
    "df_selected = df_filtered.select(\"Date\", \"To_Token_Price\")\n",
    "\n",
    "# Converter o DataFrame para Pandas para poder utilizar o TensorFlow\n",
    "df_pandas = df_selected.toPandas()\n",
    "\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "# Converter a coluna de data para o formato esperado pelo modelo LSTM\n",
    "df_pandas[\"Date\"] = pd.to_datetime(df_pandas[\"Date\"])\n",
    "\n",
    "# Normalizar os valores do preço\n",
    "max_price = df_pandas[\"To_Token_Price\"].max()\n",
    "min_price = df_pandas[\"To_Token_Price\"].min()\n",
    "df_pandas[\"To_Token_Price\"] = (df_pandas[\"To_Token_Price\"] - min_price) / (max_price - min_price)\n",
    "\n",
    "# Dividir os dados em conjunto de treinamento e teste\n",
    "train_size = int(len(df_pandas) * 0.8)\n",
    "train_data = df_pandas[:train_size]\n",
    "test_data = df_pandas[train_size:]\n",
    "\n",
    "# Preparar os dados para treinamento do modelo LSTM\n",
    "window_size = 10  # Tamanho da janela de entrada\n",
    "X_train, y_train = prepare_data(train_data[\"To_Token_Price\"], window_size)\n",
    "X_test, y_test = prepare_data(test_data[\"To_Token_Price\"], window_size)\n",
    "\n",
    "\n",
    "\n",
    "# Construir o modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(window_size, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "# Treinar o modelo LSTM\n",
    "try:\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32)\n",
    "except:\n",
    "    print(\"Não há dados suficientes para fazer as previsões. Exception\")\n",
    "\n",
    "# Fazer previsões para as próximas 10 horas\n",
    "if len(X_test) >= 5:\n",
    "    last_window = np.expand_dims(X_test[-1], axis=0)\n",
    "    predictions = []\n",
    "    for _ in range(10):\n",
    "        next_prediction = model.predict(last_window)\n",
    "        predictions.append(next_prediction[0][0])\n",
    "        last_window = np.roll(last_window, -1)\n",
    "        last_window[-1] = next_prediction\n",
    "\n",
    "    # Desnormalizar os valores das previsões\n",
    "    predictions = (np.array(predictions) * (max_price - min_price)) + min_price\n",
    "\n",
    "    # Imprimir as previsões\n",
    "    print(\"Previsões de preço para as próximas 10 horas:\")\n",
    "    print(predictions)\n",
    "else:\n",
    "    print(\"Não há dados suficientes para fazer as previsões.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1f601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As 10 moedas mais compradas:\n",
      "+---------------+-----------+\n",
      "|To_Token_Symbol|Num_Compras|\n",
      "+---------------+-----------+\n",
      "|           WETH|        908|\n",
      "|         BOBINU|        106|\n",
      "|          PEPIG|         82|\n",
      "|         PEPINU|         72|\n",
      "|        INUPEPE|         57|\n",
      "|         WOJINU|         51|\n",
      "|          pɛpeɪ|         41|\n",
      "|        PEPNEKO|         41|\n",
      "|      PEPINU2.0|         36|\n",
      "|        DOGEINU|         32|\n",
      "+---------------+-----------+\n",
      "\n",
      "As 10 moedas mais vendidas:\n",
      "+-----------------+----------+\n",
      "|From_Token_Symbol|Num_Vendas|\n",
      "+-----------------+----------+\n",
      "|             WETH|       909|\n",
      "|           BOBINU|       113|\n",
      "|           PEPINU|        63|\n",
      "|         FLOKIINU|        51|\n",
      "|          DogeInu|        50|\n",
      "|            pɛpeɪ|        49|\n",
      "|           WOJINU|        49|\n",
      "|          PEPECAT|        37|\n",
      "|          INUPEPE|        36|\n",
      "|            Shinu|        36|\n",
      "+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Filtrar as transações de compra\n",
    "compras_df = df.filter(df.To_Token_Symbol.isNotNull()) \\\n",
    "               .groupBy(\"To_Token_Symbol\") \\\n",
    "               .agg(F.count(\"*\").alias(\"Num_Compras\")) \\\n",
    "               .orderBy(F.desc(\"Num_Compras\")) \\\n",
    "               .limit(10)\n",
    "\n",
    "# Filtrar as transações de venda\n",
    "vendas_df = df.filter(df.From_Token_Symbol.isNotNull()) \\\n",
    "              .groupBy(\"From_Token_Symbol\") \\\n",
    "              .agg(F.count(\"*\").alias(\"Num_Vendas\")) \\\n",
    "              .orderBy(F.desc(\"Num_Vendas\")) \\\n",
    "              .limit(10)\n",
    "\n",
    "# Exibir as 10 moedas mais compradas\n",
    "print(\"As 10 moedas mais compradas:\")\n",
    "compras_df.show()\n",
    "\n",
    "# Exibir as 10 moedas mais vendidas\n",
    "print(\"As 10 moedas mais vendidas:\")\n",
    "vendas_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed641f66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
