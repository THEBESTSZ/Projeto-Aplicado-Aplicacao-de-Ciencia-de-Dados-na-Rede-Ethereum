{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f772d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pyspark\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, expr, collect_list, struct\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from pyspark.sql.functions import col, isnan, when, trim\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece84728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_last_n_blocks(df, atual_block_id, n):\n",
    "    min_block_id = atual_block_id - n\n",
    "    filtered_df = df.filter((df[\"Block_ID\"] >= min_block_id) & (df[\"Block_ID\"] < atual_block_id))\n",
    "    return filtered_df\n",
    "\n",
    "def get_dataframes(num_last_blocks = 50):\n",
    "    block_df = None\n",
    "    file_path = \"../Scripts/output/block/block.csv\"\n",
    "    # Ler o arquivo CSV\n",
    "    block_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    \n",
    "    swap_df = None\n",
    "    file_path = \"../Scripts/output/swap/swap_transactions.csv\"\n",
    "    # Ler o arquivo CSV\n",
    "    swap_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    \n",
    "    transaction_df = None\n",
    "    file_path = \"../Scripts/output/transaction/transactions.csv\"\n",
    "    # Ler o arquivo CSV\n",
    "    transaction_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    \n",
    "    last_block = block_df.agg(F.max(\"Block_ID\")).collect()[0][0]\n",
    "    \n",
    "    block_df = filter_last_n_blocks(block_df, last_block, num_last_blocks)\n",
    "    transaction_df = filter_last_n_blocks(transaction_df, last_block, num_last_blocks)\n",
    "    swap_df = filter_last_n_blocks(swap_df, last_block, num_last_blocks)\n",
    "    \n",
    "    return block_df, transaction_df, swap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbb1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_null(c):\n",
    "    return when(~(col(c).isNull() | isnan(col(c)) | (trim(col(c)) == \"\")), col(c))\n",
    "\n",
    "def clean_swap_dataframe(swap_df):\n",
    "    swap_df_cleaned = swap_df.select([to_null(c).alias(c) for c in swap_df.columns]).na.drop()\n",
    "\n",
    "    swap_df_cleaned = swap_df_cleaned.withColumn(\"From_Token_Price\", when(col(\"From_Token_Price\") == 0, float('nan')).otherwise(col(\"From_Token_Price\")))\n",
    "    swap_df_cleaned = swap_df_cleaned.withColumn(\"To_Token_Price\", when(col(\"To_Token_Price\") == 0, float('nan')).otherwise(col(\"To_Token_Price\")))\n",
    "    \n",
    "    return swap_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2b3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(block_df, transaction_df, swap_df_cleaned):\n",
    "    combined_altered_df = block_df.drop(\"Gas_Limit\")\n",
    "    combined_altered_df = combined_altered_df.drop(\"Gas_Used\")\n",
    "    combined_altered_df = combined_altered_df.drop(\"Timestamp_Block\")\n",
    "    df_combined = combined_altered_df.join(transaction_df, \"Block_ID\").join(swap_df_cleaned, \"Hash_Transaction\")\n",
    "    \n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7184e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"../Scripts/output\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "num_last_blocks = 5000\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Análise de Transações Ethereum\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "block_df, transaction_df, swap_df = get_dataframes(5000)\n",
    "\n",
    "swap_df_cleaned = clean_swap_dataframe(swap_df)\n",
    "df_combined = combine_dataframes(block_df, transaction_df, swap_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ff2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_pandas = df_combined.toPandas()\n",
    "df_combined_pandas.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa97edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
